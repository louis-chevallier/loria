{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Séminaire IMT Grand-Est\n",
    "\n",
    "# Introduction à l'apprentissage automatique: TP2 - Exercice 2\n",
    "\n",
    "<br>\n",
    "\n",
    "Le but de cet exercice est de classifier les textes d'un corpus de textes sur différents sujets, de manière à regrouper les textes portant sur le même sujet. La méthode utilisée (statistiques sur un _sac de mots_) admet des variantes plus efficaces que ce que l'on utilise dans ce TP; ces variantes font toujours l'objet de recherches.\n",
    "\n",
    "Nous allons utiliser un jeu de données de `scikit-learn`: des textes provenant de newsgroups (les ancètres des forums) consacrés à un sujet.\n",
    "\n",
    "L'exercice est inspiré de [la documentation scikit-learn](https://scikit-learn.org/0.18/auto_examples/text/document_clustering.html)\n",
    "\n",
    "\n",
    "On commence par charger les bibliothèques utiles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"magic function\" Jupyter pour l'affichage des graphiques dans le carnet:\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn import cluster\n",
    "\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite on charge des textes venant de quatre groupes de discussion (`categories`) provenant du dataset _20 newsgroups_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    'misc.forsale',\n",
    "    'rec.sport.baseball',\n",
    "    'comp.graphics',\n",
    "    'sci.space',\n",
    "]\n",
    "\n",
    "print(\"Loading 20newsgroups dataset for categories:\")\n",
    "print(categories)\n",
    "\n",
    "dataset = fetch_20newsgroups(subset='all', categories=categories)\n",
    "\n",
    "print(\"%d documents\" % len(dataset.data))\n",
    "print(\"%d categories\" % len(dataset.target_names))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit à l'aide des cellules suivantes que l'objet `dataset` a en particulier des attributs `target` (un entier représentant la catégorie du texte, ici entre 0 et 3 car on a extrait 4 catégories), `target_names` (les noms des catégories en anglais, dans l'ordre des entiers de `target`), et `data` (le texte des messages):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(dir(dataset))   #  dir permet de lister les attributs et méthodes d'un objet\n",
    "print(\"contenu de l'attribut target: %a\" % dataset.target)\n",
    "print(\"contenu de l'attribut target_names: %a\" % dataset.target_names)\n",
    "\n",
    "print()\n",
    "print()\n",
    "no_doc=10   # essayez avec d'autres textes parmi les 3929 chargés\n",
    "print(\"Document no: %d\" % no_doc)\n",
    "print(\"numéro de catégorie: %d\" % dataset.target[no_doc]) \n",
    "print(\"ce qui correspond au sujet: %s\" % dataset.target_names[dataset.target[no_doc]])\n",
    "print()\n",
    "print(\"Le texte est:\")\n",
    "print()\n",
    "print(dataset.data[no_doc])   # vérifiez que le texte a l'air cohérent avec le sujet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objectif est de parvenir à classifier les textes du corpus par sujet, en supposant bien entendu que l'on ne connaisse pas le newsgroup d'appartenance. On va donc chercher une classification non-supervisée sur les textes stockés dans `dataset.data`, l'information dans `dataset.target` nous servira uniquement pour vérifier à la fin le résultat de notre classification.\n",
    "\n",
    "Les algorithmes de clustering traitent des points dans un espace (vectoriel) muni d'une distance. La première étape est donc de transformer chaque texte en un vecteur.\n",
    "\n",
    "Une approche standard dans le domaine de la fouille de textes est la transformation TF-IDF, décrite [sur wikipedia](https://en.wikipedia.org/wiki/TF-IDF) (lisez le début de la page).\n",
    "\n",
    "Il y a deux étapes: \n",
    "* on considère un sous-ensemble des mots présents dans tous les textes à classifier (un \"sac de mots\", défini ci-dessous), et on calcule pour chaque texte la fréquence d'occurrences de chacun des mots du vocabulaire: à ce stade chaque texte est représenté par un vecteur de fréquences (étape TF: _term frequency_)\n",
    "* les vecteurs sont normalisés de manière à ce que les mots présents dans beaucoup de textes du corpus (donc peu discriminants) aient un poids plus faible dans le vecteur représentant un texte (étape IDF: _inverse document frequency_). Intuitivement, si un mot apparaît fréquemment dans un document, son importance pour le sujet à identifier n'est pas la même s'il est de toute façon présent dans tous les documents (indépendamment de leur sujet) ou non. \n",
    "\n",
    "Scikit-learn propose une fonction permettant de calculer la représentation TF-IDF des textes du corpus (plus de détails [ici](https://scikit-learn.org/0.18/modules/feature_extraction.html#text-feature-extraction), complément d'information). Dans la cellule suivante, on associe à chaque texte du corpus un vecteur TF-IDF. Pour former le \"sac de mots\" (_bag of words_) au préalable, on ne tient pas compte des mots trop courants (`max_df`: les mots présents dans plus de 50% des documents sont éliminés) ou trop rares (`min_df`: les mots présents dans moins de deux documents dans le corpus sont éliminés), ainsi que des mots trop courants en anglais (`stop_words`, contenus dans une liste pré-définie). De manière à limiter les temps de calcul (et éviter la malédiction de la dimensionalité, comme on le verra plus tard), on construit des vecteurs de dimension $N$ en ne gardant comme vocabulaire que les $N$ mots les plus fréquents du corpus (on n'utilise pas `min_df`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=1000\n",
    "vectorizer = TfidfVectorizer(max_features=N,max_df=0.5,stop_words='english')\n",
    "vectors = vectorizer.fit_transform(dataset.data)\n",
    "\n",
    "print(\"nombre de documents représentés %d\" %vectors.shape[0])\n",
    "print(\"nombre de mots dans le vocabulaire %d\" %vectors.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ce stade, `vectors[no_doc]` est un vecteur TF_IDF de dimension $N$ représentant le document d'indice $no_{doc}$. \n",
    "\n",
    "Le vocabulaire extrait est le suivant (essayez aussi sans `stop_words='english'` dans la cellule précédente, et différentes valeurs de $N$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A titre illustratif, le code suivant représente le vecteur associé à quatre documents: en ordonnée on voit la statistique TF-IDF, en abscisse le numéro du mot dans le \"sac de mots\". (pas besoin de comprendre la syntaxe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l'affichage peut prendre un peu de temps\n",
    "\n",
    "plt.figure();\n",
    "no_doc=10\n",
    "plt.bar(np.arange(N),np.asarray(vectors[no_doc].todense())[0],width=10);\n",
    "plt.title(\"sujet:\"+dataset.target_names[dataset.target[no_doc]]);\n",
    "\n",
    "plt.figure();\n",
    "no_doc=100\n",
    "plt.bar(np.arange(N),np.asarray(vectors[no_doc].todense())[0],width=10);\n",
    "plt.title(\"sujet:\"+dataset.target_names[dataset.target[no_doc]]);\n",
    "\n",
    "plt.figure();\n",
    "no_doc=50\n",
    "plt.bar(np.arange(N),np.asarray(vectors[no_doc].todense())[0],width=10);\n",
    "plt.title(\"sujet:\"+dataset.target_names[dataset.target[no_doc]]);\n",
    "\n",
    "plt.figure();\n",
    "no_doc=1000\n",
    "plt.bar(np.arange(N),np.asarray(vectors[no_doc].todense())[0],width=10);\n",
    "plt.title(\"sujet:\"+dataset.target_names[dataset.target[no_doc]]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "__Travail à faire__: utilisez les algorithmes de classification hiérarchique (single-linkage, Ward) et k-means pour classifier les vecteurs TF-IDF (donc les textes) en quatre classes. Les `labels` (valeurs entre 0 et 3) calculés pour chaque vecteur TF-IDF correspondent au numéro de la classe identifiée.\n",
    "\n",
    "On utilisera `AgglomerativeClustering` avec les options `linkage='single'` et `linkage='ward'` (cf __[documentation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html)__ et MiniBatchKmeans (cf __[documentation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MiniBatchKMeans.html)__)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# votre code:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observez les différences de temps de calcul (obtenus en faisant la différence entre les valeurs retournées par `time.time()`).\n",
    "\n",
    "<br>\n",
    "\n",
    "Nous cherchons à présent à valider les classifications obtenues. La difficulté dans une classification non-supervisée est que les labels calculés sont arbitraires: le label 0 d'une classification n'a pas de raison de correspondre au \"vrai\" label 0. Commencez par afficher les labels attribués par chaque méthode aux 30 premiers textes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# votre code:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On dispose des [matrices de confusion](https://scikit-learn.org/0.17/modules/generated/sklearn.metrics.confusion_matrix.html) (à lire)\n",
    "\n",
    "Que peut-on dire des résultats suivants?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(\"matrice de confusion pour 'single-linkage':\")\n",
    "print(confusion_matrix(dataset.target,labels_single))\n",
    "plt.figure()\n",
    "plt.imshow(confusion_matrix(dataset.target,labels_single))  # représentation visuelle\n",
    "plt.colorbar();\n",
    "print()\n",
    "print(\"matrice de confusion pour 'Ward':\")\n",
    "print(confusion_matrix(dataset.target,labels_ward))\n",
    "plt.figure()\n",
    "plt.imshow(confusion_matrix(dataset.target,labels_ward))  # représentation visuelle\n",
    "plt.colorbar();\n",
    "print()\n",
    "print(\"matrice de confusion pour 'KMeans':\")\n",
    "print(confusion_matrix(dataset.target,labels_KM))\n",
    "plt.figure()\n",
    "plt.imshow(confusion_matrix(dataset.target,labels_KM))  # représentation visuelle\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pour les plus rapides\n",
    "\n",
    "Testez l'algorithme avec une valeur grande de $N$ (taille du sac de mots, et aussi dimensionnalité de la représentation TF-IDF de chaque texte), et avec une valeur faible. Qu'en conclure? \n",
    "\n",
    "Essayez également d'identifier plus de quatre sujets. Représentez le _elbow plot_ pour les K-means.\n",
    "\n",
    "Etendez vos résultats à l'ensemble du jeu de données `20newsgroups`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
