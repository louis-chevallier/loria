{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Séminaire IMT Grand-Est\n",
    "\n",
    "# Introduction à l'apprentissage automatique: TP1 - Exercice 1\n",
    "\n",
    "<br>\n",
    "\n",
    "Le but de ce premier exercice est de se familiariser avec l'environnement __[Jupyter Notebook](https://jupyter-notebook.readthedocs.io/en/stable/)__ et l'utilisation de quelques fonctionalités de la bibliothèque Python __[scikit-learn](http://scikit-learn.org)__. Nous allons nous intéresser à un problème de régression classique, sous l'angle des problématiques de l'apprentissage automatique. Le carnet est adapté de __[ce tutoriel](https://ipython-books.github.io/81-getting-started-with-scikit-learn/)__.\n",
    "\n",
    "<br>\n",
    "\n",
    "Exécutez les cellules de ce carnet Jupyter les unes après les autres.\n",
    "En cas de problème d'exécution du code Python, vous pouvez redémarrer le noyau / kernel (onglet dans la barre du carnet Jupyter en haut).\n",
    "Pour ajouter vos commentaires personnels, créez une nouvelle cellule (onglet Insert) de type Markdown (à sélectionner dans le menu déroulant en haut du carnet). Un aide-mémoire pour Markdown est disponible __[ici](https://www.ibm.com/support/knowledgecenter/en/SSGNPV_1.1.3/dsx/markd-jupyter.html)__. Vous pouvez aussi examiner le code source des cellules de cette page à l'aide du \"double clic\".\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On commence par charger les bibliothèques utiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import des bibliothèques Python utiles:\n",
    "import numpy as np\n",
    "import sklearn.linear_model as lm\n",
    "from sklearn.metrics import mean_squared_error \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# \"magic function\" Jupyter pour l'affichage des graphiques dans le carnet:\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle et génération de données synthétiques\n",
    "\n",
    "<br>\n",
    "\n",
    "Dans la cellule ci-dessous, on génère des données y selon un modèle dépendant de $x\\in[0,1.5]$. Quel est ce modèle, et quel type de bruit affecte les données?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modèle génératif:\n",
    "# les x_mod et y_mod serviront uniquement pour les représentations graphiques\n",
    "x_mod = np.linspace(0., 1.5, 150)\n",
    "print(\"taille de x_mod avant reshape: \"+str(x_mod.shape))\n",
    "x_mod = x_mod.reshape(len(x_mod),1)  # scikit-learn exige une ligne par observation\n",
    "print(\"taille de x_mod après reshape: \"+str(x_mod.shape))  # que fait reshape?\n",
    "y_mod = np.exp(3*x_mod)\n",
    "\n",
    "# données:\n",
    "# les y correspondant à certaines valeurs de x sont générés selon le modèle, et on ajoute un bruit\n",
    "x_data = np.array([0, .1, .2, .5, .8, .9, 1])\n",
    "x_data = x_data.reshape(len(x_data),1) \n",
    "y_data = np.exp(3*x_data) + 2.0 * np.random.randn(len(x_data),1)\n",
    "print(\"données x_data:\")\n",
    "print(x_data)\n",
    "print(\"données y_data:\")\n",
    "print(y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La cellule suivante permet une représentation graphique des données (on parle aussi des _observations_) et du modèle génératif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# représentation graphique:\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(x_mod, y_mod, '--k')\n",
    "plt.plot(x_data, y_data, 'or', ms=10)\n",
    "plt.xlim(0, 1.5)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylim(-10, 80)\n",
    "plt.ylabel(\"y\")\n",
    "plt.title('observations et modèle génératif')\n",
    "plt.legend([\"modèle génératif\",\"observations\"]);  # rem: le \";\" évite des messages intempestifs sous Jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons à présent essayer de créer des modèles permettant de prédire des valeurs $y$ correspondant à des valeurs de $x$ entre 0 et 1.5. Bien entendu, ces modèles seront créés à partir des données $(x_{data},y_{data})$. Nous confronterons nos prédictions aux valeurs obtenues par le modèle génératif des données, mais, bien entendu, dans une application réelle, le modèle génératif serait inconnu.\n",
    "\n",
    "_Remarque_: attention, le mot \"modèle\" est utilisé dans deux sens différents...\n",
    "\n",
    "<br> \n",
    "\n",
    "Nous commençons par un modèle de régression linéaire. Cela consiste à prédire les valeurs de $y$ par une fonction affine de $x$:\n",
    "$$ y_{pred} = \\alpha_0 + \\alpha_1 x$$\n",
    "\n",
    "Les valeurs de $\\alpha_0$ et $\\alpha_1$ sont estimées par la méthode des moindres carrés: on cherche les paramètres $\\alpha_0$ et $\\alpha_1$ qui minimisent\n",
    "$$\\sum_{i=1}^n |y_{data}[i] - \\alpha_0 - \\alpha_1 x_{data}[i]|^2$$\n",
    "sur l'ensemble des $n$ observations $(x_{data}[i],y_{data}[i])_{1\\leq i\\leq N}$ (ici, $n=7$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On crée un objet scikit-learn pour la régression linéaire:\n",
    "lr = lm.LinearRegression()\n",
    "# lorsqu'on crée un objet scikit-learn, on dispose de méthodes et attributs \n",
    "# cf la documentation: on ne se servira que de quelques uns d'entre eux\n",
    "\n",
    "# On estime les paramètres alpha_0 et alpha_1: \n",
    "lr.fit(x_data, y_data)  \n",
    "# de manière générale, la méthode fit permet l'apprentissage des paramètres du modèle \n",
    "# (ici par la méthodes des moindres carrés), qui sont stockés dans les attributs suivants:\n",
    "print(lr.intercept_)  \n",
    "print(lr.coef_)\n",
    "\n",
    "# On prédit des valeurs de y pour les x entre 0 et 1.5\n",
    "y_pred_lr = lr.predict(x_mod)  \n",
    "# la méthode predict permet de prédire les valeurs y pour les x passés en argument\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelles sont les valeurs de $\\alpha_0$ et $\\alpha_1$ ? Comparez aux valeurs obtenues par votre voisin.\n",
    "\n",
    "<br>\n",
    "\n",
    "La cellule suivante permet la représentation graphique de la prédiction par régression linéaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# représentation graphique:\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(x_mod, y_mod, '--k')\n",
    "plt.plot(x_data, y_data, 'or', ms=10)\n",
    "plt.plot(x_mod, y_pred_lr, '-g')\n",
    "plt.xlim(0, 1.5)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylim(-10, 80)\n",
    "plt.ylabel(\"y\")\n",
    "plt.title('observations, modèle, et régression linéaire')\n",
    "plt.legend([\"modèle\",\"observations\",\"droite de régression\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme on le voit, la droite de régression passe globalement entre les points rouges, mais est en fait assez peu représentative du modèle génératif. Cela était prévisible, car ce modèle n'est pas linéaire.\n",
    "\n",
    "<br> \n",
    "\n",
    "La cellule suivante permet d'afficher l'erreur quadratique moyenne de prédiction sur les observations. Comment est défini cet indicateur?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"régression linéaire, MSE = \"+str(mean_squared_error(y_data,lr.predict(x_data))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons à présent construire un modèle prédictif polynomial (donc non linéaire):\n",
    "$$ y_{pred} = \\alpha_0 + \\sum_{i=1}^d \\alpha_d x^d$$\n",
    "pour un entier $d>1$.\n",
    "\n",
    "Les coefficients $\\alpha_0, \\dots, \\alpha_d$ sont toujours estimés par la méthode des moindres carrés.\n",
    "\n",
    "Pour ce faire, on utilise une régression linéaire _multivariée_: au lieu de la seule variable $x$, on introduit les variables $x, x^2, x^3,\\dots, x^d$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# création des vecteurs \"puissances\"\n",
    "x_data2=np.power(x_data,2)\n",
    "x_data3=np.power(x_data,3)\n",
    "x_data4=np.power(x_data,4)\n",
    "x_data5=np.power(x_data,5)\n",
    "x_mod2=np.power(x_mod,2)\n",
    "x_mod3=np.power(x_mod,3)\n",
    "x_mod4=np.power(x_mod,4)\n",
    "x_mod5=np.power(x_mod,5)\n",
    "\n",
    "# modèle polynomial avec $d=2$\n",
    "lrp2 = lm.LinearRegression()\n",
    "lrp2.fit(np.hstack((x_data,x_data2)),y_data)  \n",
    "# hstack permet de créer un tableau de 7 lignes (observations) et 2 colonnes (attributs x et x^2)\n",
    "print(\"régression: polynome degré 2\")\n",
    "print(lrp2.intercept_)\n",
    "print(lrp2.coef_)\n",
    "y_pred_lrp2=lrp2.predict(np.hstack((x_mod,x_mod2)))\n",
    "\n",
    "# modèle polynomial avec $d=5$\n",
    "lrp5 = lm.LinearRegression()\n",
    "lrp5.fit(np.hstack((x_data,x_data2,x_data3,x_data4,x_data5)),y_data)\n",
    "print(\"régresion: polynome degré 5\")\n",
    "print(lrp5.intercept_)\n",
    "print(lrp5.coef_)\n",
    "y_pred_lrp5=lrp5.predict(np.hstack((x_mod,x_mod2,x_mod3,x_mod4,x_mod5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notez les valeurs des coefficients des deux modèles.\n",
    "\n",
    "<br>\n",
    "\n",
    "Représentation graphique:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(x_mod, y_mod, '--k')\n",
    "plt.plot(x_data, y_data, 'or', ms=10)\n",
    "plt.plot(x_mod, y_pred_lr, '-g')\n",
    "plt.plot(x_mod, y_pred_lrp2, '-b')\n",
    "plt.plot(x_mod, y_pred_lrp5, '-c')\n",
    "plt.xlim(0, 1.5)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylim(-10, 80)\n",
    "plt.ylabel(\"y\")\n",
    "plt.title('régression polynomiale')\n",
    "plt.legend([\"modèle\",\"observations\",\"régression linéaire\",\"modèle degré 2\",\"modèle degré 5\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"régression polynomiale degré 2, MSE = %.3f\" %mean_squared_error(y_data,lrp2.predict(np.hstack((x_data,x_data2)))))\n",
    "\n",
    "print(\"régression polynomiale degré 5, MSE = %.3f\" %mean_squared_error(y_data,lrp5.predict(np.hstack((x_data,x_data2,x_data3,x_data4,x_data5)))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit que le modèle de degré 5 \"colle\" le mieux aux données mais est très mauvais pour prédire le modèle génératif. On dit qu'il y a _surapprentissage_, le modèle prédictif a de mauvaises capacités de _généralisation_. Par exemple, quelle valeur de $y$ est-elle prédite pour $x=1.2$?\n",
    "\n",
    "A l'inverse, le modèle de la régression linéaire simple est dans une situation de _sous-apprentissage_, la MSE étant plutôt élevée dans ce cas.\n",
    "\n",
    "<br>\n",
    "\n",
    "Ces notions seront l'objet des cours suivants. On voit que le surapprentissage semble venir de la complexité du modèle de degré 5: il dépend de 6 paramètres donc il n'est pas étonnant qu'il puisse passer très près des 7 observations. Néanmoins, il ne faut pas oublier qu'à cause du bruit, il n'est sans doute pas judicieux d'utiliser un modèle prédictif si bien adapté aux observations.\n",
    "\n",
    "Une manière classique de contrer le surapprentissage est de contraindre les paramètres du modèle prédictif (on dit qu'on _régularise_ le modèle).\n",
    "\n",
    "Dans le cadre de la régression, au lieu d'estimer les paramètres $\\alpha_i$ par minimisation des moindres carrés, on peut chercher à minimiser:\n",
    "$$\\sum_{i=1}^n \\left|y_{data}[i] - \\alpha_0 - \\sum_{j=1}^d \\alpha_j x_{data}[i]^j\\right|^2 + C \\sum_{j=0}^d \\alpha_j^2$$\n",
    "où $C$ est un paramètre positif (on parle d'_hyperparamètre_ car il ne fait pas partie des paramètres estimés par minimisation de la fonction précédente).\n",
    "\n",
    "On voit apparaître un compromis entre l'adéquation aux données et la valeur des paramètres $\\alpha_i$. La régression linéaire classique correspond au cas particulier $C=0$.\n",
    "\n",
    "Cette approche est la régression _ridge_.\n",
    "<br>\n",
    "\n",
    "Les cellules suivantes réalisent la régression _ridge_ pour les modèles polynomiaux de degrés 2 et 5. On utilise les objets scikit_learn `RidgeCV` qui permettent d'estimer automatiquement une valeur optimale pour l'hyperparamètre $C$ par _validation croisée_ (notion que l'on verra dans une prochaine séance).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge2 = lm.RidgeCV()\n",
    "ridge2.fit(np.hstack((x_data,x_data2)),y_data)\n",
    "print(\"ridge regression, polynome degré 2\")\n",
    "print(ridge2.intercept_)\n",
    "print(ridge2.coef_)\n",
    "y_pred_lrr2=ridge2.predict(np.hstack((x_mod,x_mod2)))\n",
    "\n",
    "ridge5 = lm.RidgeCV()\n",
    "ridge5.fit(np.hstack((x_data,x_data2,x_data3,x_data4,x_data5)),y_data)\n",
    "print(\"ridge regression, polynome degré 5\")\n",
    "print(ridge5.intercept_)\n",
    "print(ridge5.coef_)\n",
    "y_pred_lrr5=ridge5.predict(np.hstack((x_mod,x_mod2,x_mod3,x_mod4,x_mod5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment évoluent les valeurs des paramètres?\n",
    "\n",
    "Recherchez dans la documentation comment afficher la valeur de $C$ estimée automatiquement par `RidgeCV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(x_mod, y_mod, '--k')\n",
    "plt.plot(x_data, y_data, 'or', ms=10)\n",
    "plt.plot(x_mod, y_pred_lr, '-g')\n",
    "plt.plot(x_mod, y_pred_lrr2, '-b')\n",
    "plt.plot(x_mod, y_pred_lrr5, '-c')\n",
    "plt.xlim(0, 1.5)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylim(-10, 80)\n",
    "plt.ylabel(\"y\")\n",
    "plt.title('Ridge régression')\n",
    "plt.legend([\"modèle\",\"observations\",\"régression linéaire\",\"régression ridge degré 2\",\"régression ridge degré 5\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"régression ridge polynomiale degré 2, MSE = \"+str(mean_squared_error(y_data,ridge2.predict(np.hstack((x_data,x_data2))))))\n",
    "\n",
    "print(\"régression ridge polynomiale degré 5, MSE = \"+str(mean_squared_error(y_data,ridge5.predict(np.hstack((x_data,x_data2,x_data3,x_data4,x_data5))))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que peut-on dire de l'évolution de la MSE, et de la capacité de généralisation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Expérimentez le _Lasso_, décrit __[dans la documentation](https://scikit-learn.org/stable/modules/linear_model.html#lasso)__. Quelle est la différence essentielle avec la régression ridge? On utilisera __[LassoCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html#sklearn.linear_model.LassoCV)__. Comment comprenez-vous la remarque _\"the Lasso regression yields sparse models, it can thus be used to perform feature selection\"_ dans la documentation?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
